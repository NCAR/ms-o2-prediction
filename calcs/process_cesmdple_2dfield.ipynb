{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translates Steve Yeager's NCL scripts into python w/dask\n",
    "#Processes and bias corrects CESM-DPLE output and writes out as a netcdf4\n",
    "#Will work for CAM and POP fields\n",
    "#Should be able to (eventually) handle annual, seasonal, and monthly means\n",
    "#will need a separate script to handle 3D fields\n",
    "\n",
    "#I've marked in ALL CAPS places that need to be altered\n",
    "#-Liz Maroon 9/3/2018\n",
    "\n",
    "#import packages\n",
    "import dask                           #for using multiple cores \n",
    "import xarray as xr                   #for netcdf manipulation\n",
    "from dask.distributed import Client   #for distributing job across cores\n",
    "from dask_jobqueue import PBSCluster  #for cheyenne pbs scheduler\n",
    "import numpy as np                    #for numerics\n",
    "import dask.array as da               #for out-of-memory array setup\n",
    "import datetime                       #to correct time issue so get annual means right\n",
    "from collections import OrderedDict   #for setting netcdf attributes\n",
    "import os                             #these last three packages used to detect username/script location\n",
    "import pwd\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "\n",
    "import project as P\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE IS WHERE TO SET WHICH FILES TO PROCESS\n",
    "VAR='TS'# VAR='TS'\n",
    "MODEL='ATM'# MODEL='ATM' #SET HERE IF PROCESSING CAM OR POP OUTPUT - can write catches for LND/ICE later as needed\n",
    "ISEL = {} #{'z_t': 19}\n",
    "\n",
    "VARO = 'TS'\n",
    "\n",
    "WHICHMEAN='ANN'\n",
    "\n",
    "#WHERE ARE DPLE FILES CURRENTLY?\n",
    "DPLE_DIR='/glade/p_old/decpred/CESM-DPLE/'\n",
    "\n",
    "#WHERE AND WHAT DO YOU WANT TO CALL OUTPUT FILES?\n",
    "RAWDPOUT=f'{P.dirt}/maroon/CESM-DP-LE.{VARO}.{WHICHMEAN.lower()}.mean.nc'\n",
    "DRIFTOUT=f'{P.dirt}/maroon/CESM-DP-LE.{VARO}.{WHICHMEAN.lower()}.mean.drift.nc'\n",
    "ANOMOUT=f'{P.dirt}/maroon/CESM-DP-LE.{VARO}.{WHICHMEAN.lower()}.mean.anom.nc'\n",
    "\n",
    "#THINGS TO SPECIFY FOR CHEYENNE REQUEST GO HERE\n",
    "projectCode='NCGD0011'\n",
    "ncpu=36\n",
    "numNodes=2\n",
    "memory='80GB' #memory per node/worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a couple of commands to setup the dask stuff\n",
    "#dask.config.set({'distributed.dashboard.link':'http://localhost:{port}/status'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/mclong/miniconda3/envs/py3/lib/python3.6/site-packages/distributed/bokeh/core.py:56: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn('\\n' + msg)\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "USER = os.environ['USER']\n",
    "\n",
    "# Lots of arguments to this command are set in ~/.config/dask/jobqueue.yaml\n",
    "cluster = PBSCluster(queue='regular',\n",
    "                     cores = 36,\n",
    "                     processes = 9,\n",
    "                     memory = '100GB',                     \n",
    "                     project = 'NCGD0033',\n",
    "                     walltime = '04:00:00',\n",
    "                     local_directory=f'/glade/scratch/{USER}/dask-tmp')\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nnodes = 2\n",
    "cluster.scale(9*Nnodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after delayed call\n"
     ]
    }
   ],
   "source": [
    "#Make array for start years\n",
    "first_syear=1954\n",
    "last_syear=2015\n",
    "S=np.arange(first_syear+1,last_syear+1.01,1,dtype='int32')  #hacky addition of 1.01 to end year b/c roundoff weirdness\n",
    "prefix='b.e11.BDP.f09_g16.'\n",
    "\n",
    "#hacky function to correct for xarray not working well with times outside the \"allowed\" range\n",
    "def mfload(thesefiles,isel={}):\n",
    "    d1800=1800*365 #assumes noleap calender\n",
    "    if MODEL=='OCN':\n",
    "        ds=xr.open_mfdataset(thesefiles,concat_dim=\"ensemble\", chunks={\"time\": 122},decode_times=False)\n",
    "        ds.time.values = ds.time_bound.mean(ds.time_bound.dims[-1]).isel(ensemble=0)\n",
    "        ds['time'].values=ds['time'].values-d1800\n",
    "        ds['time'].attrs['units']='days since 1800-01-01 00:00:00'\n",
    "        ds['time_bound'].attrs['units']='days since 1800-01-01 00:00:00'\n",
    "        ds['time_bound'].values=ds['time_bound'].values-d1800\n",
    "        ds=xr.decode_cf(ds,decode_times=True)\n",
    "    \n",
    "    else:\n",
    "        ds=xr.open_mfdataset(thesefiles,concat_dim=\"ensemble\", chunks={\"time\": 122},decode_times=False)\n",
    "        ds.time.values = ds.time_bnds.mean(ds.time_bnds.dims[-1]).isel(ensemble=0)\n",
    "        ds=xr.decode_cf(ds,decode_times=True)\n",
    "    if isel:\n",
    "        ds=ds.isel(**isel)\n",
    "        \n",
    "    # MCL: eliminated below, taking average of time_bound above\n",
    "    #ds['time']=ds['time']-np.timedelta64(15,'D')  #subtracting 15 days from 'time'\n",
    "    #hacky correction for time vs time_bnds issue. time is for end of month, not middle, \n",
    "    #xarray treats that as the following month, not the previous one. \n",
    "    #Without correcting the annual means will be offset by 1 month \n",
    "    #Will test on POP later. I assume NCL is smarter and also reads the time_bnds.\n",
    "    return ds\n",
    "    \n",
    "#function that opens all the datasets for one start year and does the annual mean\n",
    "#will implement seasonal mean later\n",
    "def readyear(year,whichmean):\n",
    "    #reads in ensemble for one startyear\n",
    "    loadthesefiles=sorted(glob.glob(f\"{DPLE_DIR}/monthly/{VAR}/{prefix}{year}*.nc\"))\n",
    "    ds=mfload(loadthesefiles,isel=ISEL)\n",
    "    #ds=xr.open_mfdataset(f\"{DPLE_DIR}/monthly/{VAR}/{prefix}{year}*.nc\",concat_dim=\"ensemble\", chunks={\"time\": 122})\n",
    "    #chunk size is set to length of each monthly mean file (122 months)\n",
    "    #will need to change if working with daily files\n",
    "    if whichmean=='ANN':\n",
    "        #we're using the \".data\" attribute at the end to only return a dask array, not xarray\n",
    "        return (ds[VAR].groupby('time.year').mean('time')).isel(year=slice(1,11)).data\n",
    "    else:    \n",
    "        return ds[VAR].data\n",
    "\n",
    "#here's where the function for reading/meaning is called, but it is \"delayed\"\n",
    "#dask makes a graph for how to split the computation up across nodes but does not compute until\n",
    "#explicitly called later\n",
    "#the array for each start year are read into a list\n",
    "#pulling them in here as dask arrays (not xarrays) b/c want to make one big xarray of all startyears\n",
    "#and figured out how to concatenate this way first. There might be a better way to do this.\n",
    "dask_arrays=[dask.delayed(readyear)(ss,WHICHMEAN) for ss in S-1]\n",
    "print('after delayed call')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got array for dimensions\n"
     ]
    }
   ],
   "source": [
    "#because reading in rest of arrays as dask array, can't pull dim sizes and attrs from dask_arrays\n",
    "#need to open one startyear in as an xarray to grab dim sizes and attrs\n",
    "loadthesefiles=sorted(glob.glob(f\"{DPLE_DIR}/monthly/{VAR}/{prefix}1954*.nc\"))\n",
    "oneds=mfload(loadthesefiles,isel=ISEL)\n",
    "#oneds=xr.open_mfdataset(f\"{DPLE_DIR}/monthly/{VAR}/{prefix}1954*.nc\",concat_dim=\"ensemble\",chunks={\"time\": 122})\n",
    "\n",
    "#catch for ATM vs OCN lat/lon dims\n",
    "if MODEL=='ATM':\n",
    "    lat=oneds['lat']\n",
    "    lon=oneds['lon']\n",
    "elif MODEL=='OCN':\n",
    "    nlat=oneds['nlat']\n",
    "    nlon=oneds['nlon']\n",
    "mems=oneds['ensemble']\n",
    "\n",
    "#getting attributes from original file\n",
    "ncattrs=oneds.attrs\n",
    "varattrs=oneds[VAR].attrs\n",
    "dimattrs={}\n",
    "for dd in oneds.dims:\n",
    "    dimattrs[dd]=oneds[dd].attrs\n",
    "print('got array for dimensions')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did first compute step\n"
     ]
    }
   ],
   "source": [
    "#here's one of the heavy-lifting steps. The * is for the list format\n",
    "dask_arrays=dask.compute(*dask_arrays)\n",
    "print('did first compute step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacked into one big dask array\n"
     ]
    }
   ],
   "source": [
    "#The list of dask arrays is now \"stacked\" into one big dask array\n",
    "wholedaskarray=da.stack(dask_arrays,axis=0)\n",
    "print('stacked into one big dask array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made into an xarray\n"
     ]
    }
   ],
   "source": [
    "#function for turning dask array into xarray w/ dims consistent w/ past processed files\n",
    "def makedparray(daskarray,model):\n",
    "    if model=='ATM':\n",
    "        x=('lon',lon.values)\n",
    "        y=('lat',lat.values)\n",
    "    elif model=='OCN':\n",
    "        x=('nlon',nlon.values)\n",
    "        y=('nlat',nlat.values)\n",
    "    Lnew=np.arange(1,numL+1,1,dtype='int32')\n",
    "    Mnew=np.arange(1,len(mems.values)+1,1,dtype='int32')\n",
    "    newarray=xr.DataArray(daskarray.transpose([0, 2, 1, 3, 4]),\\\n",
    "              coords={'S':S,'L':Lnew,\\\n",
    "                      'M':Mnew,y[0]:y[1],x[0]:x[1]},\\\n",
    "              dims=['S','L','M', y[0],x[0]])\n",
    "    return newarray\n",
    "\n",
    "if WHICHMEAN=='ANN':\n",
    "    numL=10\n",
    "    array=makedparray(wholedaskarray,MODEL)         \n",
    "print('made into an xarray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a couple of functions for adding attributes to the dataarray\n",
    "def add_varattrs(da):\n",
    "    da.attrs=varattrs\n",
    "    for cc in da.coords:\n",
    "        for aa in dimattrs[cc]:\n",
    "            da[cc].attrs=dimattrs[cc]\n",
    "    return da\n",
    "\n",
    "def add_ncattrs(ds):\n",
    "    ds.attrs=ncattrs\n",
    "    ds.attrs['script']=os.path.basename(sys.argv[0])\n",
    "    now=datetime.datetime.now()\n",
    "    ds.attrs['history']='created by '+pwd.getpwuid(os.getuid()).pw_name+' on '+str(now)\n",
    "    return ds\n",
    "\n",
    "#preparing to turn the DataArray back into a DataSet (so it can be written out as a netcdf)\n",
    "array.name=VAR\n",
    "array.attrs=varattrs\n",
    "dimattrs['S']=OrderedDict([('long_name','start year')])\n",
    "dimattrs['L']=OrderedDict([('long_name','lead year')])\n",
    "dimattrs['M']=OrderedDict([('long_name','ensemble member')])\n",
    "array=add_varattrs(array)\n",
    "\n",
    "#turning DataArray into DataSet and adding ncattrs\n",
    "newds=array.to_dataset()\n",
    "newds=add_ncattrs(newds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading\n",
      "made an object for writing\n",
      "DP array with not-bias corrected output written out\n"
     ]
    }
   ],
   "source": [
    "#this is another .compute() slow step. More loading into memory\n",
    "newds=newds.compute()\n",
    "#this step is separate from the to_netcdf() call below b/c \"newds\" will be used for calculating drift.\n",
    "#only want to read all of this into memory once, and if append .to_netcdf() after this .compute()\n",
    "#the \"newobj\" cannot be used to calculate drift b/c it's not the right type of object\n",
    "print('finished loading')\n",
    "newobj=newds.to_netcdf(RAWDPOUT,engine='netcdf4',compute=False)\n",
    "print('made an object for writing')\n",
    "#for some reason, the netcdf write needs to be delayed (compute=False), or dask hangs\n",
    "please_ncwrite = newobj.persist()  #here's where it actually writes to disk\n",
    "print('DP array with not-bias corrected output written out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'TS' (S: 62, L: 10, lat: 192, lon: 288)>\n",
      "array([[[[      nan, ...,       nan],\n",
      "         ...,\n",
      "         [      nan, ...,       nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[220.48404, ..., 220.78227],\n",
      "         ...,\n",
      "         [249.90042, ..., 249.89986]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[      nan, ...,       nan],\n",
      "         ...,\n",
      "         [      nan, ...,       nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[      nan, ...,       nan],\n",
      "         ...,\n",
      "         [      nan, ...,       nan]]]], dtype=float32)\n",
      "Coordinates:\n",
      "  * S        (S) int32 1955 1956 1957 1958 1959 ... 2012 2013 2014 2015 2016\n",
      "  * L        (L) int32 1 2 3 4 5 6 7 8 9 10\n",
      "  * lat      (lat) float64 -90.0 -89.06 -88.12 -87.17 ... 87.17 88.12 89.06 90.0\n",
      "  * lon      (lon) float64 0.0 1.25 2.5 3.75 5.0 ... 355.0 356.2 357.5 358.8\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#DRIFT CALCULATION STARTS HERE\n",
    "#years to compare against verification\n",
    "climy0=1964\n",
    "climy1=2014\n",
    "\n",
    "#function that converts S,L to verification time for lead-time dependent climo calc\n",
    "def make_verification_time(ds):\n",
    "    ds['VER_TIME']=ds['S']+0.5+ds['L']-1\n",
    "    return ds\n",
    "\n",
    "#function to calculate drift \n",
    "def calc_drift(ds):\n",
    "    ds_ver=make_verification_time(ds)\n",
    "    vertime=ds_ver['VER_TIME']\n",
    "    var=ds_ver[VAR]\n",
    "    dummy=var.mean('M')    \n",
    "    #here's the key step that creates an array of booleans that select which variable entries are used\n",
    "    truefalse=np.squeeze([(vertime.values>climy0) & (vertime.values<(climy1+1))])\n",
    "    #if you don't believe me (I didn't for a while), uncomment the line below and \n",
    "    #it'll print which array entries are used in the lead-time dependent climatology\n",
    "    #print(truefalse)\n",
    "    dummy.values[~truefalse,:,:]=np.nan #setting entries we don't want for mean to NaN\n",
    "    print(dummy)\n",
    "    drift=dummy.mean('S') #mean across start years, climo calculated here\n",
    "    \n",
    "    biascorr=ds_ver[VAR]-drift  #anomalies calculated here\n",
    "    drift=add_varattrs(drift)   \n",
    "    biascorr=add_varattrs(biascorr)   \n",
    "    \n",
    "    return drift,biascorr\n",
    "\n",
    "#dataarrays of climo and anomalies created here:    \n",
    "drift_da,biascorr_da=calc_drift(newds)  \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written drift/climatology file\n"
     ]
    }
   ],
   "source": [
    "#dataarray->dataset->netcdf for drift+climo\n",
    "drift_ds=drift_da.persist().to_dataset(name='climo')\n",
    "drift_ds=add_ncattrs(drift_ds)\n",
    "drift_ds.attrs['climatology']=str(climy0)+\"-\"+str(climy1)+\", computed separately for each lead time\"\n",
    "driftobj=drift_ds.to_netcdf(DRIFTOUT,engine='netcdf4',compute=False)\n",
    "please_ncwrite = driftobj.persist()\n",
    "print('written drift/climatology file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written anomalies file. phew, it worked. I hope.\n"
     ]
    }
   ],
   "source": [
    "#same deal for anomalies\n",
    "biascorr_ds=biascorr_da.persist().to_dataset(name='anom')\n",
    "biascorr_ds=add_ncattrs(biascorr_ds)\n",
    "biascorr_ds.attrs['climatology']=str(climy0)+\"-\"+str(climy1)+\", computed separately for each lead time\"\n",
    "biasobj=biascorr_ds.to_netcdf(ANOMOUT,engine='netcdf4',compute=False)\n",
    "please_ncwrite = biasobj.persist()\n",
    "print('written anomalies file. phew, it worked. I hope.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
